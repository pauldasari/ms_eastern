dtsc_ga_650@eastern.edu
Instructor: Brandi (Henry) Megonigal


Upon completion of the course, students should be able to:
Effectively utilize ggplot2 for data visualization
Effectively utilize tidyverse functions for data manipulation
Use R to conduct correlation, as well as linear, multiple, and logistic regression
We will use R for Data Science, a free resource.  
*Books
https://r4ds.had.co.nz/ - This is used during the course

dtsc_ga_650@eastern.edu

# Books
## R
1. R for Data Science - https://r4ds.had.co.nz/
2. Elegant Graphics for Data Analysis ggplot2: https://ggplot2-book.org/
3. Introduction to DataScience: http://rafalab.dfci.harvard.edu/dsbook/
4. Adventures in R: https://www.adventures-in-r.com/
5. Cookbook for R: http://www.cookbook-r.com/
6. Advanced R: https://adv-r.hadley.nz/
7. ggplot tutorial: https://cedricscherer.netlify.app/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/#toc
## Stats
1. Learning Statistics with R: https://learningstatisticswithr.com/book/
2. Introduction to Statistical Learning: https://www.statlearning.com/
3. Statistical Inference for Data Science: https://leanpub.com/LittleInferenceBook/read
4. Tutorialspoint: https://www.tutorialspoint.com/statistics/index.htm

# Module 2
### Visualization with ggplot2 introduction
Visualize.Rmd
- loading libraries and viewing ggplot for mpg dataset

# Module 3
### More R Intro
- PIPE helps to streamline [covers data transformation] 
- R is taught in Tidyverse 
- Will be using https://r4ds.had.co.nz/workflow-basics.html 
### Data Import
- Uses the readr package
- Ref#: https://r4ds.had.co.nz/data-import.html
Data Transformation
- Code Grade
  install.packages("psych", dependencies = TRUE)
 
 # Module 4 and Module 5
 For Module 4 Model Building: https://r4ds.had.co.nz/model-basics.html
 Module 5: Assumptions
 - Parametric Data - based on probability distribution; it has 4 assumptions
 -  Normal distribution, homogeneity variance, interval data and independence
 ### Normality
 - Assume our sampling distribution is normal
 - Sampling distribution - In theory get a bunch of samples from the population; 
 - def: sampling distribution is the frequency distribution of sample means from the population
 - The sampling distribution mean should be equal to the population mean
 - Central Limit theorem - you have a large enough sample, that the sampling distribution will be normally distributed. 
 - Standard Error of mean - Standard deviation of sample distribution
 ### Checking Assumptions
 - Check based on Graphs, Skew and Kurtosis and Stats
 - Shapiro wilk test i.e. the normtest.W and normtest.p - used for normality
 - if p<0.5(significant) there is an indication the distributions are not normal
 - if p>0.05 distribution not signifcantly different than normal
 ### Homogeneity variance
 - as you go through levels of one variable, the variance of others should not change
 - Us Levene's test 

### Cheat Sheet
Skewness and Kurtosis
 - Positive value - more on left
 - Negative value - more on right
 - Look at skew.2SE and kurt.2SE columns
 >>> greater than 1 significant p <0.05
 >>> greater than 1.29 , p <0.01
 >>> greater than 1.65, p <0.001
Shapiro-Wilk
- P <0.05 - violation
Levene
- p <0.05 - violation

### Violations of Normality
What if things aren't normal
- Remove the case; Transform the data; change the score (next high score plus one; convert back from z-score; the mean +2SDs)

### Visualizing Assumptions
- The main ways to visualize normality is using ggplot and qqplot
- Shapiro-Wilk Test: Compares the scores in the sample to a normally distributed set of scores with the same mean and standard deviation; When significant, it means our sample is signigicantly different than the normal distribution
- When we have a non-significant wilk test and still determine your data are not normally distributed - is a True statement

### Odds and Ends

# Modul6: Correlation
we revisit correlation by building on knowledge from DTSC 550.  We highlight the contributions of several R packages, and introduce important concepts such as partial and semi-partial correlations that will be critical to understand regression.
- Appropriately use a variety of correlation types using R
- Understand partial and semi-partial correlation in preparation for regression
- Useful links
How to create a correlation matrix in R https://www.displayr.com/how-to-create-a-correlation-matrix-in-r/
Correlation does not equate to causation: https://www.tylervigen.com/spurious-correlations

### Correlation:
 - linear relation b/w 2 variables
 - - Strength of linear assocaition [ R pyschologist chart]
 - default is *pearsons* model
 - values are between -1 and 1 (strength is absolute)
### Variance and Covariance
There are 3 main ways
cor() - Stats package
cor.test() Stats package
rcorrc() Hmisc package
cor(hwData)
- cor() : Simple correlation matric with many variables
- cor.test():  provide much more details but with only 2 variables mostly like x and y
- rcorr(): Two matrices: correlation and p-values
- r^2 is the coefficient of determination
### Spearman
- Similarities with Pearson
- For non-parametric data (not normally distributed) called as rho (p like symbol)
- *First rank the scores for both x & y and then use pearson's equation

### visualizing correlation in R
library(corrgram)
corrgram(small)

library(DataExplorer)
plot_intro(examData)
Data explorer package for correlation: https://boxuancui.github.io/DataExplorer/

### Biserial and Point-Biserial Correlations
- Both types are used when one of two variables are discreet
- Distinctino: is the discrete variuable continuous or dichotomous (truly discete)

Continuous: some underlying continnum ; either a pass/fail a test
Dichotomous: No continnum ; Alive/dead
Bisereal: continuous
Point-biserial: dichotomy

uses bigbrother data
library(DT)
datatable(bbdata)

### Partial and Semi-Partial correlation
partial correlation can be done by library(ppcor)
pcor.test(small$Anxiety, small$Exam, small$Revise)
pcor.test(small$Revise, small$Exam, small$Anxiety)

### Partial and Semi-Partial correlation - Part2
semi partial correlations control for the effect of a third variable on only one of the two variables
**Partial: Looks for the unique relationship between two variables when other variables are ruled out
**Semi-partial: Useful when trying to explain the variance in on variable in particular (the outcome)
will be using spcor(small$Anxiety, small$Exam, small$Revise)

# Module7 - Linear Regression
### Regression Review
> Basics - Built on correlation
- Idea predict one of the variables - correlation = / = causation
Goals
1. Determine best model of t data
2. Use model to predict future outcome
- You could collect data on amount of excercise and cardiovascular health
- Use your data to model the relationship
- Tha model provides an equation, and can have any value of excercise entered to predict health

In regresion we can use IV/DV langauge (implying causality)

Types of regression
- Simple: One predictor predicting an outcome
- Multiple: Multiple predictore predicting an outcome
- Multivariate: multiple outcomes
- Logistic: Outcome is binary
- Ploynomial: model is nonlinear

In regression we use least squares regression
Residual Sum of squares as low as possible

Regression - basics : https://eastern.brightspace.com/d2l/le/content/74765/viewContent/1882661/View
**Book Andy Fields book describing statistics using R.

Linear regression
Multiple regression
Stadnardized regresion coefficients allow to compare across units - True
 - Good, but the only issue is in scale
library(lm.beta)
lm.beta(data_set)
### Multiple Regression  - Example

Model Fit 
- AIC - it is more nuanced and is the gold standard of adjusted measures
- BIC
### Methods of Regression
- Forced Entry - All predictors are added to the model at once
- Hierarchical Regression: Predictos are entered one by onein order by which would theoritically be most important; then models are compared using ANOVA
- Stepwise: Done purely on math
- All Subsets: tests all combinations; ols_step_all_possible(dataset)

# Logistic Regression
https://eastern.brightspace.com/d2l/le/content/74765/viewContent/1882665/View

- Predicting categorical outcomes
- Predictors can be either categorical or continous
- Binary outcomes - values lie between 0 and 1
- more than 2 categories that is multinomial logistic regression

library(appliedPredictiveModeling)
data("abalone")
head(abalone)
ab2 <- abalone %>%
filter(Type == "M" | Type =="F")
model <- glm(Y ~x, binomial(), data)
mod1 <- glm(Type ~ Diameter, binomial(), ab2)

### The Model
- Predictor is significant
- Otherwise, this looks pretty different than multiple regresssion
There are three related things, in order to obtain coefficients
1. Probablity
2. Odds
3. Log Odds
Calculate change in log-odds for a binary predictor
As the outcome is not continous, instead of predicting values we predict the probability of being in either group
P(Y) = probablity of Y occurring; e is the base of natural logarithms
### Key to logistic regression
logit(p) = log (p/1-p)
**The key to logistic regression is transforming our odds ratio into a probability, then to logit - False

### Abalone Interpretation
The coefficients are necessary, but are not sufficient for interpreting the regression
Instead the odds ratio is where we'll turn
mod0 <- glm(Type ~1, binomila(), ab2)

With no predictors the estimate is the log of logit i.e. log(p/1-p) = .15623
In Summary - We can obtain the coefficient witih no predictor by:
1. Finding the probablity of being 1 (Frequency of 1/total N)
2. Calculating the odds (Probability /(1-probability)
3. Calculating the log-odds i.e log(odds)

### Assessing the Model: Deviance
-In linear reg we compared observed vs predicted values; we'll do the same thing using the log-likelihood
-This is conceptually analagous to residual sum of squares
-The larger the lop likelihood, the worse the fit

- deviance = -2*log-likelihood = -2LL
- Need to access the deviance score
- log-likelihood follows a chi-square distribution
- - We can compoare our null and model deviance scores, along with their respective dfs, and compute the chi-square statistic
- If the value is significant, we believe our new model fits better than the null model

### Assessing the Model: AIC
-In multiple regression we saw the AIC as a useful statistic for model fit
-we see it here again in a different form i.e. AIC = -2LL + 2k
-Using AIC we can compare multiple models, but as we increase the number of predictors the AIC k penalty will increase 
- In this we use Rings
- There appears to be little difference between the two, despite the addition of the significant predictor, this is because of the penalty for said predictor
- AIC does not peanlize models for increasing predictors - Is a false statement

### Assessing the Model: z-values and R squared
MR, the fir of each was calculated using coeffieceints and standard errors to obtain t-statistic, well do a similar thing but use z-statistics
Z= b/SEb

sum.mod1$coefficients[,1]
R2  - There no direct comparison for logistic regression

### Another Example
library(caret)
data(Germancredit)

### One Binary Predictor Logistic Regression
### Binary Predictor Interpretation [ to be reviewed]
The regression coefficient represents the increase in log odds of having good credit with a one-unit increase in ForeignWorked

### Logistic Regression with One Continuous Predictor
1. Run the regression
2. Plug coefficients in our logistic regression equation
3. calculate probablity for x1 = 0
4. Calculate the odds for x1 = 0
5. Calculate the probability for x1 = 1
6. Calculate the odds for x1 = 1
7. Calculate the change in odds
8. Calculate the log of that change

This will be equal to our coefficient

r cheat sheet
