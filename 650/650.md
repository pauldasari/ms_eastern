dtsc_ga_650@eastern.edu
Instructor: Brandi (Henry) Megonigal


Upon completion of the course, students should be able to:
Effectively utilize ggplot2 for data visualization
Effectively utilize tidyverse functions for data manipulation
Use R to conduct correlation, as well as linear, multiple, and logistic regression
We will use R for Data Science, a free resource.  
*Books
https://r4ds.had.co.nz/ - This is used during the course

dtsc_ga_650@eastern.edu

# Books
## R
1. R for Data Science - https://r4ds.had.co.nz/
2. Elegant Graphics for Data Analysis ggplot2: https://ggplot2-book.org/
3. Introduction to DataScience: http://rafalab.dfci.harvard.edu/dsbook/
4. Adventures in R: https://www.adventures-in-r.com/
5. Cookbook for R: http://www.cookbook-r.com/
6. Advanced R: https://adv-r.hadley.nz/
7. ggplot tutorial: https://cedricscherer.netlify.app/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/#toc
## Stats
1. Learning Statistics with R: https://learningstatisticswithr.com/book/
2. Introduction to Statistical Learning: https://www.statlearning.com/
3. Statistical Inference for Data Science: https://leanpub.com/LittleInferenceBook/read
4. Tutorialspoint: https://www.tutorialspoint.com/statistics/index.htm

# Module 2
### Visualization with ggplot2 introduction
Visualize.Rmd
- loading libraries and viewing ggplot for mpg dataset

# Module 3
### More R Intro
- PIPE helps to streamline [covers data transformation] 
- R is taught in Tidyverse 
- Will be using https://r4ds.had.co.nz/workflow-basics.html 
### Data Import
- Uses the readr package
- Ref#: https://r4ds.had.co.nz/data-import.html
Data Transformation
- Code Grade
  install.packages("psych", dependencies = TRUE)
 
 # Module 4 and Module 5
 For Module 4 Model Building: https://r4ds.had.co.nz/model-basics.html
 Module 5: Assumptions
 - Parametric Data - based on probability distribution; it has 4 assumptions
 -  Normal distribution, homogeneity variance, interval data and independence
 ### Normality
 - Assume our sampling distribution is normal
 - Sampling distribution - In theory get a bunch of samples from the population; 
 - def: sampling distribution is the frequency distribution of sample means from the population
 - The sampling distribution mean should be equal to the population mean
 - Central Limit theorem - you have a large enough sample, that the sampling distribution will be normally distributed. 
 - Standard Error of mean - Standard deviation of sample distribution
 ### Checking Assumptions
 - Check based on Graphs, Skew and Kurtosis and Stats
 - Shapiro wilk test i.e. the normtest.W and normtest.p - used for normality
 - if p<0.5(significant) there is an indication the distributions are not normal
 - if p>0.05 distribution not signifcantly different than normal
 ### Homogeneity variance
 - as you go through levels of one variable, the variance of others should not change
 - Us Levene's test 

### Cheat Sheet
Skewness and Kurtosis
 - Positive value - more on left
 - Negative value - more on right
 - Look at skew.2SE and kurt.2SE columns
 >>> greater than 1 significant p <0.05
 >>> greater than 1.29 , p <0.01
 >>> greater than 1.65, p <0.001
Shapiro-Wilk
- P <0.05 - violation
Levene
- p <0.05 - violation

### Violations of Normality
What if things aren't normal
- Remove the case; Transform the data; change the score (next high score plus one; convert back from z-score; the mean +2SDs)

### Visualizing Assumptions
- The main ways to visualize normality is using ggplot and qqplot
- Shapiro-Wilk Test: Compares the scores in the sample to a normally distributed set of scores with the same mean and standard deviation; When significant, it means our sample is signigicantly different than the normal distribution
- When we have a non-significant wilk test and still determine your data are not normally distributed - is a True statement

### Odds and Ends

# Modul6: Correlation
we revisit correlation by building on knowledge from DTSC 550.  We highlight the contributions of several R packages, and introduce important concepts such as partial and semi-partial correlations that will be critical to understand regression.
- Appropriately use a variety of correlation types using R
- Understand partial and semi-partial correlation in preparation for regression
- Useful links
How to create a correlation matrix in R https://www.displayr.com/how-to-create-a-correlation-matrix-in-r/
Correlation does not equate to causation: https://www.tylervigen.com/spurious-correlations

### Correlation:
 - linear relation b/w 2 variables
 - - Strength of linear assocaition [ R pyschologist chart]
 - default is *pearsons* model
 - values are between -1 and 1 (strength is absolute)
### Variance and Covariance
There are 3 main ways
cor() - Stats package
cor.test() Stats package
rcorrc() Hmisc package
cor(hwData)
- cor() : Simple correlation matric with many variables
- cor.test():  provide much more details but with only 2 variables mostly like x and y
- rcorr(): Two matrices: correlation and p-values
- r^2 is the coefficient of determination
### Spearman
- Similarities with Pearson
- For non-parametric data (not normally distributed) called as rho (p like symbol)
- *First rank the scores for both x & y and then use pearson's equation

### visualizing correlation in R
library(corrgram)
corrgram(small)

library(DataExplorer)
plot_intro(examData)
Data explorer package for correlation: https://boxuancui.github.io/DataExplorer/

### Biserial and Point-Biserial Correlations
- Both types are used when one of two variables are discreet
- Distinctino: is the discrete variuable continuous or dichotomous (truly discete)

Continuous: some underlying continnum ; either a pass/fail a test
Dichotomous: No continnum ; Alive/dead
Bisereal: continuous
Point-biserial: dichotomy

uses bigbrother data
library(DT)
datatable(bbdata)

### Partial and Semi-Partial correlation
partial correlation can be done by library(ppcor)
pcor.test(small$Anxiety, small$Exam, small$Revise)
pcor.test(small$Revise, small$Exam, small$Anxiety)

### Partial and Semi-Partial correlation - Part2
semi partial correlations control for the effect of a third variable on only one of the two variables
**Partial: Looks for the unique relationship between two variables when other variables are ruled out
**Semi-partial: Useful when trying to explain the variance in on variable in particular (the outcome)
will be using spcor(small$Anxiety, small$Exam, small$Revise)

# Module7 - Linear Regression
### Regression Review
> Basics - Built on correlation
- Idea predict one of the variables - correlation = / = causation
Goals
1. Determine best model of t data
2. Use model to predict future outcome
- You could collect data on amount of excercise and cardiovascular health
- Use your data to model the relationship
- Tha model provides an equation, and can have any value of excercise entered to predict health

In regresion we can use IV/DV langauge (implying causality)

Types of regression
- Simple: One predictor predicting an outcome
- Multiple: Multiple predictore predicting an outcome
- Multivariate: multiple outcomes
- Logistic: Outcome is binary
- Ploynomial: model is nonlinear

In regression we use least squares regression
Residual Sum of squares as low as possible

Regression - basics : https://eastern.brightspace.com/d2l/le/content/74765/viewContent/1882661/View
**Book Andy Fields book describing statistics using R.

Linear regression
Multiple regression
Stadnardized regresion coefficients allow to compare across units - True
 - Good, but the only issue is in scale
library(lm.beta)
lm.beta(data_set)
### Multiple Regression  - Example

Model Fit 
- AIC - it is more nuanced and is the gold standard of adjusted measures
- BIC
### Methods of Regression
- Forced Entry - All predictors are added to the model at once
- Hierarchical Regression: Predictos are entered one by onein order by which would theoritically be most important; then models are compared using ANOVA
- Stepwise: Done purely on math
- All Subsets: tests all combinations; ols_step_all_possible(dataset)
